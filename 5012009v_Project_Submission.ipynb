{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPicZDNvVJlIrhD3zTqjVsN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jameskch-collab/ADALL_github/blob/main/5012009v_Project_Submission.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkekYigEVivM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import r2_score, mean_absolute_error\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings('ignore')\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "\n",
        "\"\"\"\n",
        "================================================================================\n",
        "PROJECT: AI-DRIVEN SALES ANALYTICS - COFFEE SHOP OPERATIONS\n",
        "================================================================================\n",
        "This script combines all phases of the project as outlined in the presentation:\n",
        "1. Data Cleaning & Preparation (Slide 6)\n",
        "2. Feature Engineering (Slide 7)\n",
        "3. Data Visualization (Slide 8 & 13)\n",
        "4. Model Training & Evaluation (Slide 9, 11, 12)\n",
        "\n",
        "Dataset: https://www.kaggle.com/datasets/ahmedabbas757/coffee-sales\n",
        "================================================================================\n",
        "\"\"\"\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# SECTION 1: DATA PREPARATION & CLEANING (Slide 6)\n",
        "# ------------------------------------------------------------------------------\n",
        "def clean_and_prepare_data(df):\n",
        "    print(\"\\n[Step 1] Cleaning & Formatting Data...\")\n",
        "\n",
        "    # Standardize datetime formats\n",
        "    df['datetime'] = pd.to_datetime(df['transaction_date'] + ' ' + df['transaction_time'])\n",
        "\n",
        "    # Handle potential data entry errors (Slide 10)\n",
        "    # Removing records with non-positive prices or quantities\n",
        "    df = df[(df['unit_price'] > 0) & (df['transaction_qty'] > 0)]\n",
        "\n",
        "    # Calculate revenue\n",
        "    df['revenue'] = df['unit_price'] * df['transaction_qty']\n",
        "\n",
        "    print(\"[Step 2] Aggregating Transactions into Hourly Buckets...\")\n",
        "    # Group individual transactions into hourly buckets per location\n",
        "    hourly_data = df.groupby([\n",
        "        df['datetime'].dt.floor('h'),\n",
        "        'store_location'\n",
        "    ]).agg({\n",
        "        'revenue': 'sum',\n",
        "        'product_category': 'first'\n",
        "    }).reset_index()\n",
        "\n",
        "    return hourly_data\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# SECTION 2: FEATURE ENGINEERING (Slide 7)\n",
        "# ------------------------------------------------------------------------------\n",
        "def engineer_features(df):\n",
        "    print(\"[Step 3] Feature Engineering: Capturing the Rhythm of the Shop...\")\n",
        "\n",
        "    # Time-based features\n",
        "    df['hour'] = df['datetime'].dt.hour\n",
        "    df['day_of_week'] = df['datetime'].dt.dayofweek\n",
        "    df['month'] = df['datetime'].dt.month\n",
        "\n",
        "    # Contextual features\n",
        "    df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
        "\n",
        "    # Categorical Encoding\n",
        "    le = LabelEncoder()\n",
        "    df['store_location_encoded'] = le.fit_transform(df['store_location'])\n",
        "\n",
        "    return df\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# SECTION 3: DATA VISUALIZATION (Slide 8 & 13)\n",
        "# ------------------------------------------------------------------------------\n",
        "def create_visualizations(df):\n",
        "    print(\"\\n[Step 4] Generating Visualizations...\")\n",
        "\n",
        "    # 1. Hourly Sales Trends (Slide 8)\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.lineplot(data=df, x='hour', y='revenue', ci=None, marker='o', color='#6F4E37')\n",
        "    plt.title('Hourly Sales Trends: Identifying Peaks and Slumps', fontsize=15)\n",
        "    plt.xlabel('Hour of Day', fontsize=12)\n",
        "    plt.ylabel('Average Hourly Revenue', fontsize=12)\n",
        "    plt.xticks(range(0, 24))\n",
        "    plt.grid(True, linestyle='--', alpha=0.7)\n",
        "    plt.savefig('hourly_sales_trends.png')\n",
        "    print(\" -> Saved: hourly_sales_trends.png\")\n",
        "\n",
        "    # 2. Sales by Day of Week\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    day_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
        "    sns.barplot(data=df, x='day_of_week', y='revenue', palette='copper')\n",
        "    plt.title('Sales Volume by Day of Week', fontsize=15)\n",
        "    plt.xticks(ticks=range(7), labels=day_names)\n",
        "    plt.savefig('sales_by_day.png')\n",
        "    print(\" -> Saved: sales_by_day.png\")\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# SECTION 4: MODELLING & PERFORMANCE (Slide 9, 11, 12)\n",
        "# ------------------------------------------------------------------------------\n",
        "def train_random_forest(df):\n",
        "    print(\"\\n[Step 5] Training Random Forest Model (Slide 9)...\")\n",
        "\n",
        "    features = ['hour', 'day_of_week', 'month', 'is_weekend', 'store_location_encoded']\n",
        "    X = df[features]\n",
        "    y = df['revenue']\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Random Forest Regressor (Slide 9)\n",
        "    model = RandomForestRegressor(n_estimators=100, max_depth=15, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Model Performance (Slide 11)\n",
        "    y_pred = model.predict(X_test)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "    print(\"\\n--- Model Performance Result (Slide 11) ---\")\n",
        "    print(f\"Predictive Accuracy (R-Squared): {r2:.2f}\")\n",
        "    print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
        "\n",
        "    # Key Drivers: Feature Importance (Slide 12)\n",
        "    print(\"\\n--- Key Drivers: Feature Importance (Slide 12) ---\")\n",
        "    importances = model.feature_importances_\n",
        "    feat_importance = pd.Series(importances, index=features).sort_values(ascending=False)\n",
        "    print(feat_importance)\n",
        "\n",
        "    # Visualize Feature Importance\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    feat_importance.plot(kind='barh', color='#8B4513')\n",
        "    plt.title('Key Drivers of Sales: Feature Importance', fontsize=15)\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.savefig('feature_importance.png')\n",
        "    print(\"\\n -> Saved: feature_importance.png\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# MAIN EXECUTION\n",
        "# ------------------------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # Load your data here:\n",
        "    # raw_df = pd.read_csv('Coffee Shop Sales.csv')\n",
        "\n",
        "    # FOR DEMONSTRATION: Generating synthetic data that mimics the Kaggle dataset\n",
        "    print(\"Generating synthetic data for demonstration...\")\n",
        "    np.random.seed(42)\n",
        "    n_rows = 5000\n",
        "    demo_df = pd.DataFrame({\n",
        "        'transaction_date': pd.date_range(start='2023-01-01', periods=n_rows, freq='h').strftime('%Y-%m-%d'),\n",
        "        'transaction_time': pd.date_range(start='2023-01-01', periods=n_rows, freq='h').strftime('%H:%M:%S'),\n",
        "        'unit_price': np.random.uniform(3.0, 10.0, n_rows),\n",
        "        'transaction_qty': np.random.randint(1, 5, n_rows),\n",
        "        'store_location': np.random.choice(['Downtown', 'Uptown', 'Old Town'], n_rows),\n",
        "        'product_category': np.random.choice(['Coffee', 'Tea', 'Bakery'], n_rows)\n",
        "    })\n",
        "\n",
        "    # Add patterns to make the model meaningful\n",
        "    # 1. Peak morning/lunch hours\n",
        "    times = pd.to_datetime(demo_df['transaction_time'])\n",
        "    multiplier = np.where((times.dt.hour >= 7) & (times.dt.hour <= 9), 2.5, 1.0)\n",
        "    multiplier = np.where((times.dt.hour >= 12) & (times.dt.hour <= 13), 2.0, multiplier)\n",
        "    demo_df['transaction_qty'] = (demo_df['transaction_qty'] * multiplier).astype(int)\n",
        "\n",
        "    # Pipeline execution\n",
        "    cleaned_data = clean_and_prepare_data(demo_df)\n",
        "    final_data = engineer_features(cleaned_data)\n",
        "    create_visualizations(final_data)\n",
        "    train_random_forest(final_data)"
      ]
    }
  ]
}